{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40151cce-fda5-4cfe-ba00-d49c4be48361",
   "metadata": {},
   "source": [
    "# Combine SHMI data for a parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e539b878-ee93-47a8-a5ff-dbe7998da622",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def combine_and_sum_csv(folder_path, column_index):\n",
    "    \n",
    "    csv_files = [f for f in os.listdir(folder_path) if f.endswith(\".csv\")]\n",
    "    \n",
    "    dfs = []\n",
    "\n",
    "    for file in csv_files:\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        df[\"Datum\"] = pd.to_datetime(df[\"Datum\"], format='%Y-%m-%d')\n",
    "    \n",
    "        df = df[df[\"Datum\"].dt.year >= 2019]\n",
    "        \n",
    "        dfs.append(df)\n",
    "    combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    if column_index >= len(combined_df.columns):\n",
    "        print(f\"Invalid column index {column_index}. There are only {len(combined_df.columns)} columns.\")\n",
    "        return\n",
    "    \n",
    "    column_name = combined_df.columns[column_index]\n",
    "\n",
    "    grouped_df = combined_df.groupby('Datum')[column_name].sum()\n",
    "\n",
    "    folder_name = os.path.basename(folder_path)\n",
    "    output_file_name = f\"consolidated_{folder_name}.csv\"\n",
    "\n",
    "    output_file = os.path.join(\"smhi_data_2022-today\", output_file_name)\n",
    "\n",
    "    grouped_df.to_csv(output_file)\n",
    "\n",
    "    print(f\"Saved to: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21750182-b50f-4ad5-a880-ec48bb517aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to: smhi_data_2022-today\\consolidated_parameter_2.csv\n"
     ]
    }
   ],
   "source": [
    "combine_and_sum_csv(r\"smhi_data_2022-today/parameter_2\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c51a6522-3bce-4b2a-bee1-97acf03e0785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to: smhi_data_2022-today\\consolidated_parameter_5.csv\n"
     ]
    }
   ],
   "source": [
    "combine_and_sum_csv(r\"smhi_data_2022-today/parameter_5\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a1f5a9a-922a-4fbe-9ac2-1f848d1cdb8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to: smhi_data_2022-today\\consolidated_parameter_8.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sneha\\AppData\\Local\\Temp\\ipykernel_15444\\3476833433.py:34: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  combined_df = pd.concat(dfs, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "combine_and_sum_csv(r\"smhi_data_2022-today/parameter_8\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "304e5637-4091-47bf-83f8-73de54ac5c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def combine_and_sum_solskenstid_from_2019(folder_path):\n",
    "    csv_files = [f for f in os.listdir(folder_path) if f.endswith(\".csv\")]\n",
    "    dfs = []\n",
    "\n",
    "    for file in csv_files:\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        df[\"Datum\"] = pd.to_datetime(df[\"Datum\"], errors='coerce')\n",
    "        \n",
    "        df = df[df[\"Datum\"].dt.year >= 2019]\n",
    "        \n",
    "        df[\"Tid (UTC)\"] = pd.to_datetime(df[\"Tid (UTC)\"], format='%H:%M:%S').dt.strftime('%H:%M:%S')\n",
    "        \n",
    "        dfs.append(df)\n",
    "    combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    grouped_df = combined_df.groupby(['Datum', 'Tid (UTC)'])['Solskenstid'].sum().reset_index()\n",
    "\n",
    "    folder_name = os.path.basename(folder_path)\n",
    "    output_file_name = f\"consolidated_{folder_name}.csv\"\n",
    "    output_file = os.path.join(\"smhi_data_2022-today\", output_file_name)\n",
    "\n",
    "    grouped_df.to_csv(output_file, index=False)\n",
    "\n",
    "    print(f\"Saved to: {output_file}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2103a78d-f491-4518-af16-3b483d1ad115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to: smhi_data_2022-today\\consolidated_parameter_10.csv\n"
     ]
    }
   ],
   "source": [
    "combine_and_sum_solskenstid_from_2019(r\"smhi_data_2022-today/parameter_10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4911b531-c52f-4e00-b069-1964cc2e2c66",
   "metadata": {},
   "source": [
    "# Converting day values to hourly values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "946a5966-3357-47ab-8a39-1edbb8798056",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def convert_daily_to_hourly(folder_path, file_name, column_index):\n",
    "\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    df[\"Datum\"] = pd.to_datetime(df[\"Datum\"], errors=\"coerce\")\n",
    "\n",
    "    invalid_dates = df[df[\"Datum\"].isna()]\n",
    "\n",
    "    df = df.dropna(subset=[\"Datum\"])\n",
    "\n",
    "    if \"parameter_2\" in file_name.lower():\n",
    "        interpolated_column_name = \"Air temperature\"\n",
    "    elif \"parameter_5\" in file_name.lower():\n",
    "        interpolated_column_name = \"Precipitation\"\n",
    "    elif \"parameter_8\" in file_name.lower():\n",
    "        interpolated_column_name = \"Snow depth\"\n",
    "    else:\n",
    "        interpolated_column_name = \"Interpolated Value\"\n",
    "\n",
    "    hourly_df = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        day = row[\"Datum\"]\n",
    "        value = row.iloc[column_index] \n",
    "\n",
    "        for hour in range(24):\n",
    "            hourly_time = day + timedelta(hours=hour)\n",
    "\n",
    "            datum = hourly_time.strftime('%d-%m-%Y') \n",
    "            tid_utc = hourly_time.strftime('%H:%M:%S')\n",
    "\n",
    "            hourly_df.append({\n",
    "                \"Datum\": datum,\n",
    "                \"Tid (UTC)\": tid_utc,\n",
    "                interpolated_column_name: value\n",
    "            })\n",
    "\n",
    "    hourly_df = pd.DataFrame(hourly_df)\n",
    "\n",
    "    # Save the hourly data to a new CSV file\n",
    "    output_file_name = f\"hourly_{file_name}\"\n",
    "    output_file_path = os.path.join(folder_path, output_file_name)\n",
    "    hourly_df.to_csv(output_file_path, index=False)\n",
    "\n",
    "    print(f\"Hourly data saved to: {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4bf126ea-0fdb-44d0-9010-5535a241e186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw 'Datum' values:\n",
      "0    2019-01-01\n",
      "1    2019-01-02\n",
      "2    2019-01-03\n",
      "3    2019-01-04\n",
      "4    2019-01-05\n",
      "5    2019-01-06\n",
      "6    2019-01-07\n",
      "7    2019-01-08\n",
      "8    2019-01-09\n",
      "9    2019-01-10\n",
      "Name: Datum, dtype: object\n",
      "Parsed 'Datum' values:\n",
      "0   2019-01-01\n",
      "1   2019-01-02\n",
      "2   2019-01-03\n",
      "3   2019-01-04\n",
      "4   2019-01-05\n",
      "5   2019-01-06\n",
      "6   2019-01-07\n",
      "7   2019-01-08\n",
      "8   2019-01-09\n",
      "9   2019-01-10\n",
      "Name: Datum, dtype: datetime64[ns]\n",
      "Processed data:\n",
      "       Datum  Lufttemperatur\n",
      "0 2019-01-01           274.1\n",
      "1 2019-01-02           -24.0\n",
      "2 2019-01-03          -102.6\n",
      "3 2019-01-04           141.1\n",
      "4 2019-01-05            76.4\n",
      "Hourly data saved to: smhi_data_2022-today\\hourly_consolidated_parameter_2.csv\n"
     ]
    }
   ],
   "source": [
    "convert_daily_to_hourly(r\"smhi_data_2022-today\", \"consolidated_parameter_2.csv\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "09a2b98c-743f-4de6-80f2-65668760caa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw 'Datum' values:\n",
      "0    2019-01-01\n",
      "1    2019-01-02\n",
      "2    2019-01-03\n",
      "3    2019-01-04\n",
      "4    2019-01-05\n",
      "5    2019-01-06\n",
      "6    2019-01-07\n",
      "7    2019-01-08\n",
      "8    2019-01-09\n",
      "9    2019-01-10\n",
      "Name: Datum, dtype: object\n",
      "Parsed 'Datum' values:\n",
      "0   2019-01-01\n",
      "1   2019-01-02\n",
      "2   2019-01-03\n",
      "3   2019-01-04\n",
      "4   2019-01-05\n",
      "5   2019-01-06\n",
      "6   2019-01-07\n",
      "7   2019-01-08\n",
      "8   2019-01-09\n",
      "9   2019-01-10\n",
      "Name: Datum, dtype: datetime64[ns]\n",
      "Processed data:\n",
      "       Datum  Nederbördsmängd\n",
      "0 2019-01-01             22.3\n",
      "1 2019-01-02             22.2\n",
      "2 2019-01-03             37.0\n",
      "3 2019-01-04             88.3\n",
      "4 2019-01-05             13.4\n",
      "Hourly data saved to: smhi_data_2022-today\\hourly_consolidated_parameter_5.csv\n"
     ]
    }
   ],
   "source": [
    "convert_daily_to_hourly(r\"smhi_data_2022-today\", \"consolidated_parameter_5.csv\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5caa84d1-0574-4af8-902e-753eefac2093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw 'Datum' values:\n",
      "0    2019-01-01\n",
      "1    2019-01-02\n",
      "2    2019-01-03\n",
      "3    2019-01-04\n",
      "4    2019-01-05\n",
      "5    2019-01-06\n",
      "6    2019-01-07\n",
      "7    2019-01-08\n",
      "8    2019-01-09\n",
      "9    2019-01-10\n",
      "Name: Datum, dtype: object\n",
      "Parsed 'Datum' values:\n",
      "0   2019-01-01\n",
      "1   2019-01-02\n",
      "2   2019-01-03\n",
      "3   2019-01-04\n",
      "4   2019-01-05\n",
      "5   2019-01-06\n",
      "6   2019-01-07\n",
      "7   2019-01-08\n",
      "8   2019-01-09\n",
      "9   2019-01-10\n",
      "Name: Datum, dtype: datetime64[ns]\n",
      "Processed data:\n",
      "       Datum  Snödjup\n",
      "0 2019-01-01     1.22\n",
      "1 2019-01-02     1.15\n",
      "2 2019-01-03     1.34\n",
      "3 2019-01-04     1.28\n",
      "4 2019-01-05     1.00\n",
      "Hourly data saved to: smhi_data_2022-today\\hourly_consolidated_parameter_8.csv\n"
     ]
    }
   ],
   "source": [
    "convert_daily_to_hourly(r\"smhi_data_2022-today\", \"consolidated_parameter_8.csv\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a5a8b1c8-9261-4305-a8c6-7cd987b08038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing 'Datum' column in combined_output.csv: 'Datum'\n",
      "Warning: Missing Sunshine time data for the following dates after 13-01-2019:\n",
      "            Date      Time\n",
      "288   2019-01-13  00:00:00\n",
      "289   2019-01-13  01:00:00\n",
      "290   2019-01-13  02:00:00\n",
      "291   2019-01-13  03:00:00\n",
      "292   2019-01-13  04:00:00\n",
      "...          ...       ...\n",
      "51859 2024-11-30  19:00:00\n",
      "51860 2024-11-30  20:00:00\n",
      "51861 2024-11-30  21:00:00\n",
      "51862 2024-11-30  22:00:00\n",
      "51863 2024-11-30  23:00:00\n",
      "\n",
      "[31673 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sneha\\AppData\\Local\\Temp\\ipykernel_15444\\1946782504.py:81: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  final_combined_df[\"Sunshine time\"] = final_combined_df[\"Sunshine time\"].fillna(method='ffill')  # Forward fill\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined CSV saved to: Hourly_data\\combined_output.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def combine_csv_by_datum_and_tid(folder_path, output_file_name):\n",
    "    \n",
    "    csv_files = [f for f in os.listdir(folder_path) if f.endswith(\".csv\")]\n",
    "    final_combined_df = pd.DataFrame()\n",
    "    \n",
    "    for file in csv_files:\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        df = pd.read_csv(file_path)\n",
    "        df[\"Datum\"] = pd.to_datetime(df[\"Datum\"], errors='coerce', dayfirst=True)\n",
    "        df[\"Tid (UTC)\"] = pd.to_datetime(df[\"Tid (UTC)\"], errors='coerce', format='%H:%M:%S')\n",
    "        df[\"Date\"] = df[\"Datum\"].dt.strftime('%Y-%m-%d')\n",
    "        df[\"Time\"] = df[\"Tid (UTC)\"].dt.strftime('%H:%M:%S')\n",
    "        \n",
    "        df = df.drop(columns=[\"Datum\", \"Tid (UTC)\"])\n",
    "        df = df.rename(columns={\"Solskenstid\": \"Sunshine time\"})\n",
    "        \n",
    "        if final_combined_df.empty:\n",
    "            final_combined_df = df\n",
    "        else:\n",
    "            final_combined_df = pd.merge(final_combined_df, df, on=[\"Date\", \"Time\"], how=\"outer\")\n",
    "\n",
    "    final_combined_df = final_combined_df.groupby([\"Date\", \"Time\"]).first().reset_index()\n",
    "\n",
    "    final_combined_df = final_combined_df.sort_values(by=[\"Date\", \"Time\"]).reset_index(drop=True)\n",
    "\n",
    "    final_combined_df[\"Date\"] = pd.to_datetime(final_combined_df[\"Date\"], errors='coerce')\n",
    "\n",
    "    missing_sunshine_data = final_combined_df[(final_combined_df[\"Sunshine time\"].isna()) & \n",
    "                                              (final_combined_df[\"Date\"] >= pd.to_datetime(\"2019-01-13\"))]\n",
    "\n",
    "    if not missing_sunshine_data.empty:\n",
    "        print(missing_sunshine_data[['Date', 'Time']])\n",
    "\n",
    "    final_combined_df[\"Sunshine time\"] = final_combined_df[\"Sunshine time\"].fillna(method='ffill')\n",
    "\n",
    "    output_file_path = os.path.join(folder_path, output_file_name)\n",
    "    final_combined_df.to_csv(output_file_path, index=False)\n",
    "    \n",
    "    print(f\"Combined CSV saved to: {output_file_path}\")\n",
    "\n",
    "\n",
    "combine_csv_by_datum_and_tid(r\"Hourly_data\", \"combined_output.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ee9204-1f1c-4b82-b0f3-c5ad09f7e0c3",
   "metadata": {},
   "source": [
    "# Combine SHMI data with electricity price data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "1e54c491-508a-4dd2-9bd6-11d2fa1d1b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined CSV saved to: electricity\\combined_electricity_2019_2024.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def combine_electricity_data(folder_path, output_file_name):\n",
    "    csv_files = [f for f in os.listdir(folder_path) if f.startswith(\"SE3\") and f.endswith(\".csv\") and any(year in f for year in ['2019', '2020', '2021', '2022', '2023', '2024'])]\n",
    "\n",
    "    df_list = []\n",
    "    \n",
    "    for file in csv_files:\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        df[\"Date\"] = df[\"MTU (CET/CEST)\"].str.split(' ', expand=True)[0]\n",
    "        df[\"Time\"] = df[\"MTU (CET/CEST)\"].str.split(' ', expand=True)[1]\n",
    "        \n",
    "        df = df.drop(columns=[\"MTU (CET/CEST)\"])\n",
    "        \n",
    "        df[\"Date\"] = pd.to_datetime(df[\"Date\"], format='%d.%m.%Y', errors='coerce')\n",
    "        \n",
    "        df_list.append(df)\n",
    "\n",
    "    final_combined_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "    final_combined_df = final_combined_df.sort_values(by=[\"Date\", \"Time\"]).reset_index(drop=True)\n",
    "    \n",
    "    output_file_path = os.path.join(folder_path, output_file_name)\n",
    "    final_combined_df.to_csv(output_file_path, index=False)\n",
    "    \n",
    "    print(f\"Combined CSV saved to: {output_file_path}\")\n",
    "\n",
    "combine_electricity_data(r\"electricity\", \"combined_electricity_2019_2024.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "16a17d50-45a4-4789-a222-20cc72631c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sneha\\AppData\\Local\\Temp\\ipykernel_15444\\2586221271.py:25: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  electricity_df['Time'] = pd.to_datetime(electricity_df['Time'], errors='coerce').dt.strftime('%H:%M:%S')\n",
      "C:\\Users\\sneha\\AppData\\Local\\Temp\\ipykernel_15444\\2586221271.py:26: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  weather_df['Time'] = pd.to_datetime(weather_df['Time'], errors='coerce').dt.strftime('%H:%M:%S')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged CSV saved to: merged_output.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def merge_electricity_with_weather(electricity_file, weather_file, output_file_name):\n",
    "    electricity_df = pd.read_csv(electricity_file)\n",
    "    weather_df = pd.read_csv(weather_file)\n",
    "\n",
    "    electricity_df['Date'] = pd.to_datetime(electricity_df['Date'], errors='coerce').dt.strftime('%Y-%m-%d')\n",
    "    weather_df['Date'] = pd.to_datetime(weather_df['Date'], errors='coerce').dt.strftime('%Y-%m-%d')\n",
    "    \n",
    "    electricity_df['Time'] = pd.to_datetime(electricity_df['Time'], errors='coerce').dt.strftime('%H:%M:%S')\n",
    "    weather_df['Time'] = pd.to_datetime(weather_df['Time'], errors='coerce').dt.strftime('%H:%M:%S')\n",
    "    \n",
    "    electricity_df['Date'] = electricity_df['Date'].str.strip()\n",
    "    electricity_df['Time'] = electricity_df['Time'].str.strip()\n",
    "    weather_df['Date'] = weather_df['Date'].str.strip()\n",
    "    weather_df['Time'] = weather_df['Time'].str.strip()\n",
    "\n",
    "    electricity_df = electricity_df.rename(columns={\"Day-ahead Price [EUR/MWh]\": \"price\"})\n",
    "\n",
    "    merged_df = pd.merge(weather_df, electricity_df[[\"Date\", \"Time\", \"price\"]],\n",
    "                         on=[\"Date\", \"Time\"], how=\"left\")\n",
    "    \n",
    "\n",
    "    merged_df = merged_df.sort_values(by=[\"Date\", \"Time\"]).reset_index(drop=True)\n",
    "    \n",
    "    output_file_path = output_file_name\n",
    "    merged_df.to_csv(output_file_path, index=False)\n",
    "    \n",
    "    print(f\"Merged CSV saved to: {output_file_path}\")\n",
    "\n",
    "merge_electricity_with_weather(\n",
    "    r\"electricity/combined_electricity_2019_2024.csv\", \n",
    "    r\"final_combined.csv\", \n",
    "    \"merged_output.csv\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3eb79e-46d3-4fb2-bb1f-ac39c969cac8",
   "metadata": {},
   "source": [
    "# Final data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c534bc3d-a5e4-437c-abe3-7e31c0a9835b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data cleaned successfully. Saved as 'final_cleaned.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"final_combined.csv\")\n",
    "\n",
    "\n",
    "df = df.dropna()\n",
    "\n",
    "\n",
    "df['Price'] = pd.to_numeric(df['Price'], errors='coerce')\n",
    "df = df.dropna(subset=['Price'])\n",
    "\n",
    "\n",
    "df.to_csv(\"final_cleaned.csv\", index=False)\n",
    "\n",
    "print(\"Data cleaned successfully. Saved as 'final_cleaned.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f3d72c-a12f-49af-b7af-3855a489eea3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
